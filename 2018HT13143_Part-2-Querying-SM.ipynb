{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This program is used to query the index with search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk as nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.sparse import coo_matrix\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decalre names of all files generated in part 2A\n",
    "vocabulary_index_file_path = \".\\\\data\\\\vocabulary.pkl\"\n",
    "document_index_file_path = \".\\\\data\\\\file_index.pkl\"\n",
    "inverted_index_file_path = \".\\\\data\\inverted_index.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search queries are in following block. Change them as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the input search queries\n",
    "common_nouns_query = \"Irish language\"#\"American writer\" #\"church of england\"\n",
    "proper_nouns_query = \"Joy and Lewis\"\n",
    "rare_terms_query = \"tuomas Cosmos holopainen sojourner lakeview Testament\"\n",
    "ambiguous_query = \"Chinese England Testament\"\n",
    "complex_query = \"Chinese writer sitting at Church of England\"\n",
    "\n",
    "queries = [common_nouns_query, proper_nouns_query, rare_terms_query, ambiguous_query, complex_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read vocabulary index\n",
    "pickle_file = open(vocabulary_index_file_path, \"rb\")\n",
    "vocabulary_dict = pickle.load(pickle_file)\n",
    "pickle_file.close()\n",
    "\n",
    "# Read document index\n",
    "pickle_file = open(document_index_file_path, \"rb\")\n",
    "document_dict = pickle.load(pickle_file)\n",
    "pickle_file.close()\n",
    "\n",
    "# Read term frequency matrix\n",
    "pickle_file = open(inverted_index_file_path, \"rb\")\n",
    "inverted_index_matrix = pickle.load(pickle_file)\n",
    "pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words :  42040\n"
     ]
    }
   ],
   "source": [
    "total_number_of_unique_words = len(vocabulary_dict)\n",
    "print(\"Total number of unique words : \", total_number_of_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert the document index, we will need this to find document id's in the last step\n",
    "indexed_docs = dict()\n",
    "for doc_id, index in document_dict.items():\n",
    "    indexed_docs[index] = doc_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## methods for calculating lnc and ltc scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating LTC for the query\n",
    "def calc_ltc(document, indexes):\n",
    "    # calculate log tf of lnc score for the documents\n",
    "    vector_length = len(document)\n",
    "    log_tf_value = np.zeros(vector_length)\n",
    "    log_tf_value[indexes] = 1 + np.log10(document[indexes])\n",
    "\n",
    "    # calculate idf of lnc score\n",
    "    idf_value = np.ones(vector_length)\n",
    "    total_number_of_documents = inverted_index_matrix.shape[1]\n",
    "    #print(\"total num docs=\", total_number_of_documents)\n",
    "    for token_index in indexes:\n",
    "        relevent_number_of_documents = inverted_index_matrix.getrow(token_index).count_nonzero()\n",
    "        idf_value[token_index] = np.log10(total_number_of_documents/relevent_number_of_documents)\n",
    "    #print(\"idf_value[indexes]=\", idf_value[indexes])\n",
    "    # calcuate weights\n",
    "    weights = log_tf_value * idf_value\n",
    "\n",
    "    # perform cosine normalization\n",
    "    sum_of_squares = sum(weights * weights)\n",
    "    cosine_theta = 1 / np.sqrt(sum_of_squares)\n",
    "\n",
    "    length_norm_values = weights * cosine_theta\n",
    "    return length_norm_values\n",
    "\n",
    "# calculating LNC for the document\n",
    "def calc_lnc(document, indexes):\n",
    "    # calculate log tf of lnc score for the documents\n",
    "    vector_length = len(document)\n",
    "    log_tf_value = np.zeros(vector_length)\n",
    "    for index in indexes:\n",
    "        if document[index] > 0:\n",
    "            log_tf_value[index] = 1+np.log10(document[index])\n",
    "        else:\n",
    "            log_tf_value[index] = 0\n",
    "\n",
    "    # calculate idf of lnc score\n",
    "    idf_value = np.ones(vector_length)\n",
    "\n",
    "    # calcuate weights\n",
    "    weights = log_tf_value * idf_value\n",
    "    #print(\"weights=\", weights[indexes])\n",
    "    # perform cosine normalization\n",
    "    sum_of_squares = sum(weights[indexes] * weights[indexes])\n",
    "    #print(\"sum_of_squares=\", sum_of_squares)\n",
    "    cosine_theta = 90\n",
    "    if sum_of_squares > 0:\n",
    "        cosine_theta = 1 / np.sqrt(sum_of_squares)\n",
    "    #print(\"cosine_theta=\", cosine_theta)\n",
    "    \n",
    "    length_norm_values = weights * cosine_theta\n",
    "    return length_norm_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## following block processes the queries and prints the output for each query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching documents for query =  irish language\n",
      "Document with ID =  5813  has relevancy score of  0.9999490723489033\n",
      "Document with ID =  5830  has relevancy score of  0.999501739384538\n",
      "Document with ID =  6501  has relevancy score of  0.9985334533915411\n",
      "Document with ID =  6514  has relevancy score of  0.9971121745966887\n",
      "Document with ID =  6561  has relevancy score of  0.9935252057683959\n",
      "Document with ID =  6532  has relevancy score of  0.9784680085744129\n",
      "Document with ID =  6643  has relevancy score of  0.9784680085744129\n",
      "Document with ID =  6677  has relevancy score of  0.9784680085744129\n",
      "Document with ID =  6546  has relevancy score of  0.9646280018786675\n",
      "Document with ID =  6678  has relevancy score of  0.9217706352224505\n",
      "\n",
      "\n",
      "Searching documents for query =  joy and lewis\n",
      "Document with ID =  5813  has relevancy score of  0.7252621949553562\n",
      "Document with ID =  6652  has relevancy score of  0.3900677404351355\n",
      "Document with ID =  6533  has relevancy score of  0.37657739061234785\n",
      "Document with ID =  6612  has relevancy score of  0.31378455638702435\n",
      "Document with ID =  6435  has relevancy score of  0.3046273703385062\n",
      "Document with ID =  6710  has relevancy score of  0.29159880412268546\n",
      "Document with ID =  6548  has relevancy score of  0.2396450829597231\n",
      "Document with ID =  6611  has relevancy score of  0.2334129129459465\n",
      "Document with ID =  6503  has relevancy score of  0.21647998140672642\n",
      "Document with ID =  6719  has relevancy score of  0.19096135342427664\n",
      "\n",
      "\n",
      "Searching documents for query =  tuomas cosmos holopainen sojourner lakeview testament\n",
      "Document with ID =  6824  has relevancy score of  0.8839406808357468\n",
      "Document with ID =  5813  has relevancy score of  0.40903563314936753\n",
      "Document with ID =  5820  has relevancy score of  0.31949317505106717\n",
      "Document with ID =  6459  has relevancy score of  0.31949317505106717\n",
      "Document with ID =  6516  has relevancy score of  0.31949317505106717\n",
      "Document with ID =  6424  has relevancy score of  0.27033129948792606\n",
      "Document with ID =  6542  has relevancy score of  0.27033129948792606\n",
      "Document with ID =  6599  has relevancy score of  0.27033129948792606\n",
      "Document with ID =  6704  has relevancy score of  0.27033129948792606\n",
      "Document with ID =  6728  has relevancy score of  0.27033129948792606\n",
      "\n",
      "\n",
      "Searching documents for query =  chinese england testament\n",
      "Document with ID =  6728  has relevancy score of  0.8917379197847396\n",
      "Document with ID =  6424  has relevancy score of  0.8480719439861031\n",
      "Document with ID =  6599  has relevancy score of  0.8420552243552075\n",
      "Document with ID =  6704  has relevancy score of  0.8294330683370081\n",
      "Document with ID =  6469  has relevancy score of  0.821754715107013\n",
      "Document with ID =  6542  has relevancy score of  0.8034957616138372\n",
      "Document with ID =  6824  has relevancy score of  0.8034957616138372\n",
      "Document with ID =  6586  has relevancy score of  0.8034957616138371\n",
      "Document with ID =  5813  has relevancy score of  0.762834479612875\n",
      "Document with ID =  6449  has relevancy score of  0.5952551518224758\n",
      "\n",
      "\n",
      "Searching documents for query =  chinese writer sitting at church of england\n",
      "Document with ID =  6675  has relevancy score of  0.4353884243805586\n",
      "Document with ID =  5820  has relevancy score of  0.42041801385341837\n",
      "Document with ID =  5813  has relevancy score of  0.4094599107577134\n",
      "Document with ID =  6751  has relevancy score of  0.38582975237536643\n",
      "Document with ID =  6466  has relevancy score of  0.3499401753280543\n",
      "Document with ID =  6629  has relevancy score of  0.33168156927064607\n",
      "Document with ID =  6533  has relevancy score of  0.3288402196020319\n",
      "Document with ID =  6469  has relevancy score of  0.3261293846726105\n",
      "Document with ID =  6814  has relevancy score of  0.3207568075021997\n",
      "Document with ID =  6449  has relevancy score of  0.3195141741577744\n",
      "\n",
      "\n",
      "Search Completed!\n"
     ]
    }
   ],
   "source": [
    "# for each query, search the top 10 relevent documents\n",
    "for query in queries:\n",
    "    query = query.lower()\n",
    "    query_tokens = word_tokenize(query)\n",
    "    query_indexes = []\n",
    "    query_doc = np.zeros(total_number_of_unique_words)\n",
    "    for token in query_tokens:\n",
    "        if vocabulary_dict.get(token) is None:\n",
    "            #print(\"vocabulary \", token, \" is not preseent \")\n",
    "            continue\n",
    "        if vocabulary_dict.get(token) not in query_indexes:\n",
    "            #print(\"vocabulary \", token, \" is at index \", vocabulary_dict.get(token))\n",
    "            query_indexes.append(vocabulary_dict.get(token))\n",
    "        query_doc[vocabulary_dict.get(token)] =  query_doc[vocabulary_dict.get(token)] + 1\n",
    "    # find ltc for query\n",
    "    ltc_array = calc_ltc(query_doc, query_indexes)[query_indexes]\n",
    "    print(\"Searching documents for query = \", query)\n",
    "    #print(query_doc[query_indexes])\n",
    "\n",
    "    # find similarity with all documents\n",
    "    relevency_array = []\n",
    "    for i in range (0, inverted_index_matrix.shape[1]):\n",
    "        raw_tf_array = inverted_index_matrix.getcol(i).toarray().reshape(total_number_of_unique_words)\n",
    "        doc_to_match = np.zeros(total_number_of_unique_words)\n",
    "        doc_to_match[query_indexes] = raw_tf_array[query_indexes]\n",
    "        doc_to_match = np.zeros(total_number_of_unique_words)\n",
    "        #print(raw_tf_array[query_indexes])\n",
    "        doc_to_match[query_indexes] = np.nan_to_num(raw_tf_array[query_indexes])\n",
    "        # calculating LNC for the document\n",
    "        lnc_array = calc_lnc(doc_to_match, query_indexes)[query_indexes]\n",
    "        product_array = lnc_array * ltc_array\n",
    "        #print(\"Product = \", product_array)\n",
    "        relevency = sum(product_array)\n",
    "        relevency_array.append((indexed_docs[i], relevency))\n",
    "\n",
    "    sorted_relevency_array = sorted(relevency_array, key=lambda sim: sim[1], reverse = True)\n",
    "    \n",
    "    # Show relevancy score for top 10 documents\n",
    "    for doc_id, score in sorted_relevency_array[:10]:\n",
    "        print(\"Document with ID = \", doc_id, \" has relevancy score of \", score)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"Search Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
